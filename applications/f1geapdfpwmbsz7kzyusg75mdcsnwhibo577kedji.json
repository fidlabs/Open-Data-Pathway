{
  "Version": "1.3",
  "ID": "f1geapdfpwmbsz7kzyusg75mdcsnwhibo577kedji",
  "Issue Number": "148",
  "Client": {
    "Name": "Harvard LIL / Filecoin Foundation",
    "Region": "United States",
    "Industry": "Government",
    "Website": "data.gov",
    "Social Media": "ianconsolata",
    "Social Media Type": "Slack",
    "Role": "Data Preparer"
  },
  "Project": {
    "Brief history of your project and organization": "The Filecoin Foundation works with many different social impact projects. We have our own allocator where we issue DC to our partners, but I am applying here myself because I want to directly test the usability of various onboarding tools with the data.gov dataset, and I cannot use our allocator to allocate DC to myself.",
    "Is this project associated with other projects/ecosystem stakeholders?": "Yes",
    "Describe the data being stored onto Filecoin": "This is a regularly updated mirror of all data files linked from data.gov.\n\nThe repository is maintained by the Harvard Law School Library Innovation Lab as part of our project to preserve U.S. federal public data.\n\nCollection Format\nEach dataset on data.gov has a unique slug known as its name. We store each dataset in this repository as:\n\ncollections/data_gov/<name>/<version>.zip\nWe also store a metadata file for each dataset in the metadata directory:\n\nmetadata/data_gov/<name>/<version>.json\n<version> is a v followed by the number of times we have downloaded the dataset (v1, v2, etc.)\n\nFor example, the data.gov dataset https://catalog.data.gov/dataset/fruit-and-vegetable-prices is stored in this repository as:\n\ncollections/data_gov/fruit-and-vegetable-prices/v1.zip\nmetadata/data_gov/fruit-and-vegetable-prices/v1.json\nDataset Format\nEach dataset zip file is a BagIt package created by our bag-nabit tool.\n\nBagIt is a simple file format, established by the Library of Congress, consisting of a folder of metadata and text files. Our BagIt files follow this directory structure:\n\ndata/\nfiles/:\n...: these are the actual files you likely want to use as a researcher, downloaded from the data.gov listing.\nheaders.warc: request and response headers from HTTP fetches for files in files/\nsigned-metadata.json: metadata including data.gov's API description of the dataset\nThe bags also contain these files, which are useful for authenticating the provenance of the data:\n\nbagit.txt: standard BagIt file\nbag-info.txt: standard BagIt file\nmanifest-sha256.txt: standard BagIt file\ntagmanifest-sha256.txt: standard BagIt file\nsignatures/: directory of signature files\nMetadata File Format\nEach metadata JSON file contains three main sections:\n\nbag_info: Contains the BagIt metadata including:\n\nBag-Software-Agent: The version of nabit used to create the archive\nBagging-Date: When the archive was created\nsigned_metadata: Contains detailed information about the dataset including:\n\nid: A UUID for this specific archive\nurl: The data.gov URL for the dataset\ndescription: A brief description including the dataset title and creating organization\ndata_gov_metadata: The complete metadata from data.gov's API, including:\nDataset details (title, description, etc.)\nOrganization information\nResource listings\nTags and other metadata\ncollection_tasks: Records of the HTTP requests made to collect the dataset\nzip_entries: Listing of each entry in the collection zip file, which can be used to fetch individual files from the zip file via range request without downloading the entire archive.\n\nRollup files\nThere are several rollup files at the top level to help with finding datasets of interest:\n\nmetadata.csv.zip: CSV listing the name, organization, title, date, metadata path, and collection path for each dataset\nmetadata.jsonl.zip: JSON lines file with complete metadata for each dataset, including the signed_metadata and zip_entries sections (equivalent to downloading the metadata/ directory as a single file)\nfile_listing.jsonl.zip: zipped JSON lines file showing the s3 listing of all files in the repository\nDownloading data\nTo download an individual dataset by name you can construct its URL, such as:\n\nhttps://source.coop/harvard-lil/gov-data/collections/data_gov/fruit-and-vegetable-prices/v1.zip\nhttps://source.coop/harvard-lil/gov-data/metadata/data_gov/fruit-and-vegetable-prices/v1.json\nTo download large numbers of files, we recommend the aws or rclone command line tools:\n\naws s3 cp s3://us-west-2.opendata.source.coop/harvard-lil/gov-data/collections/data_gov/<name>/v1.zip --no-sign-request\nData Limitations\ndata.gov includes multiple kinds of datasets, including some that link to actual data files, such as CSV files, and some that link to HTML landing pages. Our process runs a \"shallow crawl\" that collects only the directly linked files. Datasets that link only to a landing page will need to be collected separately.\n\nSource code\nThe source code used to generate this and other repositories is available at https://github.com/harvard-lil/data-vault. We welcome conversation and collaboration in the issue tracker for that project.\n\nCollection Dates and Update Schedule\nFiles in this repository were collected intermittently between 2024-11-19 and 2025-02-06.\n\nBeginning on 2025-02-06, we will update the repository daily.",
    "Where was the data currently stored in this dataset sourced from": "AWS Cloud",
    "How do you plan to prepare the dataset": "I will try a variety of different tools in order to test different pathways. I will start with Singularity and RIBS, and welcome suggestions for other tools or onramps to evaluate.",
    "Please share a sample of the data (a link to a file, an image, a table, etc., are good ways to do this.)": "https://source.coop/harvard-lil/gov-data",
    "Confirm that this is a public dataset that can be retrieved by anyone on the network (i.e., no specific permissions or access rights are required to view the data)": "[x] I confirm",
    "What is the expected retrieval frequency for this data": "Sporadic",
    "For how long do you plan to keep this dataset stored on Filecoin": "Permanently",
    "In which geographies do you plan on making storage deals": "North America, Europe, South America, Africa, Asia other than Greater China, Greater China, Australia (continent), Antarctica",
    "How will you be distributing your data to storage providers": "Others",
    "Please list the provider IDs and location of the storage providers you will be working with. Note that it is a requirement to list a minimum of 5 unique provider IDs, and that your client address will be verified against this list in the future": "No clue yet, happy to work with any SPs who are interested and can provide strong retrievals.",
    "Can you confirm that you will follow the Fil+ guideline (Data owner should engage at least 4 SPs and no single SP ID should receive >30% of a client's allocated DataCap)": "Yes"
  },
  "Datacap": {
    "Type": "ldn-v3",
    "Data Type": "Slingshot",
    "Total Requested Amount": "80",
    "Single Size Dataset": "18",
    "Replicas": 4,
    "Weekly Allocation": "20"
  },
  "Lifecycle": {
    "State": "Submitted",
    "Validated At": "",
    "Validated By": "",
    "Active": true,
    "Updated At": "2025-04-10 17:48:21.207750121 UTC",
    "Active Request ID": "",
    "On Chain Address": "f1geapdfpwmbsz7kzyusg75mdcsnwhibo577kedji",
    "Multisig Address": "false",
    "edited": false
  },
  "Allocation Requests": []
}